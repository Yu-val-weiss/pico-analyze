analysis_name: "tiny-lora-acts-analysis"

metrics: 
  - metric_name: cka
    target_checkpoint: 20_100
    data_split: "val"
    components: 
      - component_name: lora_ov_circuit
        data_type: "activations"
        layer_suffixes: 
          output_layer: "attention.o_proj"
          value_layer: "attention.v_proj"
        layers: [0,1,2,3,4,5,6,7,8,9,10,11]
      - component_name: lora
        data_type: "activations"
        layer_suffixes: "swiglu.w_2"
        layers: [0,1,2,3,4,5,6,7,8,9,10,11]


  - metric_name: condition_number
    data_split: "val"
    components:
      - component_name: lora
        data_type: "activations"
        layer_suffixes: "attention.v_proj"
        layers: [0,1,2,3,4,5,6,7,8,9,10,11] 
      - component_name: lora
        data_type: "activations"
        layer_suffixes: "attention.o_proj"
        layers: [0,1,2,3,4,5,6,7,8,9,10,11] 
      - component_name: lora
        data_type: "activations"
        layer_suffixes: "swiglu.w_2"
        layers: [0,1,2,3,4,5,6,7,8,9,10,11]
      - component_name: lora_ov_circuit
        data_type: "activations"
        layer_suffixes: 
          output_layer: "attention.o_proj"
          value_layer: "attention.v_proj"
        layers: [0,1,2,3,4,5,6,7,8,9,10,11]

  - metric_name: gini
    data_split: "val"
    components:
      - component_name: lora
        data_type: "activations"
        layer_suffixes: "attention.v_proj"
        layers: [0,1,2,3,4,5,6,7,8,9,10,11] 
      - component_name: lora
        data_type: "activations"
        layer_suffixes: "attention.o_proj"
        layers: [0,1,2,3,4,5,6,7,8,9,10,11] 
      - component_name: lora
        data_type: "activations"
        layer_suffixes: "swiglu.w_2"
        layers: [0,1,2,3,4,5,6,7,8,9,10,11]
      - component_name: lora_ov_circuit
        data_type: "activations"
        layer_suffixes: 
          output_layer: "attention.o_proj"
          value_layer: "attention.v_proj"
        layers: [0,1,2,3,4,5,6,7,8,9,10,11]

  - metric_name: hoyer
    data_split: "val"
    components:
      - component_name: lora
        data_type: "activations"
        layer_suffixes: "attention.v_proj"
        layers: [0,1,2,3,4,5,6,7,8,9,10,11] 
      - component_name: lora
        data_type: "activations"
        layer_suffixes: "attention.o_proj"
        layers: [0,1,2,3,4,5,6,7,8,9,10,11] 
      - component_name: lora
        data_type: "activations"
        layer_suffixes: "swiglu.w_2"
        layers: [0,1,2,3,4,5,6,7,8,9,10,11]
      - component_name: lora_ov_circuit
        data_type: "activations"
        layer_suffixes: 
          output_layer: "attention.o_proj"
          value_layer: "attention.v_proj"
        layers: [0,1,2,3,4,5,6,7,8,9,10,11]

  - metric_name: norm
    data_split: "val"
    norm_type: "frobenius"
    components:
      - component_name: lora
        data_type: "activations"
        layer_suffixes: "attention.v_proj"
        layers: [0,1,2,3,4,5,6,7,8,9,10,11] 
      - component_name: lora
        data_type: "activations"
        layer_suffixes: "attention.o_proj"
        layers: [0,1,2,3,4,5,6,7,8,9,10,11] 
      - component_name: lora
        data_type: "activations"
        layer_suffixes: "swiglu.w_2"
        layers: [0,1,2,3,4,5,6,7,8,9,10,11]
      - component_name: lora_ov_circuit
        data_type: "activations"
        layer_suffixes: 
          output_layer: "attention.o_proj"
          value_layer: "attention.v_proj"
        layers: [0,1,2,3,4,5,6,7,8,9,10,11]

steps: 
  start: 0
  end: 20_100
  step: 100

monitoring: 
  output_dir: "analysis_results"

  save_to_wandb: true
  wandb:
    entity: "pico-lm"
    project: "pico-relora"
